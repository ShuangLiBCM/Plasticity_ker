{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Augment the literature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from modelval import pairptl, network, trainer, dataset, data_aug_knn, perform_eval\n",
    "from modelval.ArbDataGen import arb_w_gen, data_Gen\n",
    "from modelval.spk_visu import spk_see, raster\n",
    "from modelval import gp_regressor\n",
    "from modelval import data_aug_gp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modelval.kernel import KernelGen\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptl_idx</th>\n",
       "      <th>pre_spk_num</th>\n",
       "      <th>pre_spk_freq</th>\n",
       "      <th>post_spk_num</th>\n",
       "      <th>post_spk_freq</th>\n",
       "      <th>ptl_occ</th>\n",
       "      <th>ptl_freq</th>\n",
       "      <th>dt1</th>\n",
       "      <th>dt2</th>\n",
       "      <th>dt3</th>\n",
       "      <th>dw_mean</th>\n",
       "      <th>dw_ste</th>\n",
       "      <th>train_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-102.898046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.322590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-75.579896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.674768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-80.871473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-12.696449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-67.562239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.231446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-63.553410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.990216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ptl_idx  pre_spk_num  pre_spk_freq  post_spk_num  post_spk_freq  ptl_occ  \\\n",
       "0        1            1             0             1              0       60   \n",
       "1        1            1             0             1              0       60   \n",
       "2        1            1             0             1              0       60   \n",
       "3        1            1             0             1              0       60   \n",
       "4        1            1             0             1              0       60   \n",
       "\n",
       "   ptl_freq         dt1  dt2  dt3    dw_mean  dw_ste  train_len  \n",
       "0       1.0 -102.898046  0.0    0  -1.322590     0.0         60  \n",
       "1       1.0  -75.579896  0.0    0   2.674768     0.0         60  \n",
       "2       1.0  -80.871473  0.0    0 -12.696449     0.0         60  \n",
       "3       1.0  -67.562239  0.0    0   0.231446     0.0         60  \n",
       "4       1.0  -63.553410  0.0    0  -0.990216     0.0         60  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data frame\n",
    "data = pd.read_csv('/src/Plasticity_Ker/data/kernel_training_data_auto.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate cv for training and validation data given the test fold\n",
    "data_gen_train, data_gen_vali, data_gen_test, y_train, y_vali, y_test = data_Gen(data_aug='raw_data', test_fold_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 13), (20, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen_vali[0].shape, y_vali[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial validation mse=139.95515\n",
      "Global Step 0150 and Step 0150: validation mse=186.09557\n",
      "Global Step 0200 and Step 0200: validation mse=194.05298\n",
      "Global Step 0250 and Step 0250: validation mse=195.50922\n"
     ]
    }
   ],
   "source": [
    "# Visualize kernel\n",
    "vali_err = np.zeros(len(data_gen_train))\n",
    "kernel_pre = []\n",
    "kernel_post = []\n",
    "kernel_post_post = []\n",
    "fc_w = []\n",
    "bias = []\n",
    "scale = []\n",
    "\n",
    "for i in range(len(data_gen_train)):\n",
    "    ker_test = KernelGen()\n",
    "    \n",
    "    len_kernel = 101\n",
    "    len_trip = 151\n",
    "    ker_test = KernelGen(len_kernel=len_kernel, len_trip=len_trip)\n",
    "    \n",
    "    # Generat the spike trains and targets for STDP\n",
    "    data_hippo = data[data['ptl_idx']<5]\n",
    "    ptl_list = [1,2,3,4]\n",
    "    spk_len = int(data_hippo['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "    if_noise = 0\n",
    "    aug_times = [1,1,1,1]\n",
    "    spk_pairs_train, targets_train = arb_w_gen(df=data_gen_train[i], ptl_list=ptl_list, targets=y_train[i], if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times, seed=723)\n",
    "    spk_pairs_vali, targets_vali = arb_w_gen(df=data_gen_vali[i], ptl_list=ptl_list, targets=y_vali[i], if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times, seed=606)\n",
    "    \n",
    "    # Create the network\n",
    "    ground_truth_init = 0\n",
    "    reg_scale=(10,50,100,200)\n",
    "    init_seed=(4,5,6,7,8)\n",
    "\n",
    "    toy_data_net = network.TripNet(kernel=ker_test, ground_truth_init=ground_truth_init, reg_scale=reg_scale, n_input=spk_pairs_train.shape[1])\n",
    "    \n",
    "    # Create the trainer\n",
    "    save_dir= '/src/Plasticity_Ker/model/Trip_ptl1_4_real_aug_gp_mean_noise_alpha1_alpha3_10_long_post_post_diff_bias' + 'cv' + str(i)\n",
    "    toy_net_trainer = trainer.Trainer(toy_data_net.mse, toy_data_net.loss, input_name=toy_data_net.inputs, target_name=toy_data_net.target, save_dir=save_dir, optimizer_config={'learning_rate': toy_data_net.lr})\n",
    "    \n",
    "    # Package the data\n",
    "    train_data = dataset.Dataset(spk_pairs_train, targets_train)\n",
    "    vali_data = dataset.Dataset(spk_pairs_vali, targets_vali)\n",
    "    \n",
    "    # Learn the kernel from random initialization\n",
    "    learning_rate = 0.001\n",
    "    iterations = 5\n",
    "    min_error = -1\n",
    "    for _ in range(iterations):\n",
    "        mini_vali_loss = toy_net_trainer.train(train_data, vali_data, batch_size=128, min_error=min_error, feed_dict={toy_data_net.lr: learning_rate})\n",
    "        learning_rate = learning_rate/3\n",
    " \n",
    "    vali_err[i] = mini_vali_loss\n",
    "    toy_net_trainer.restore_best()\n",
    "    kernel_pre.append(toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre))\n",
    "    kernel_post.append(toy_net_trainer.evaluate(ops=toy_data_net.kernel_post))\n",
    "    kernel_post_post.append(toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post))\n",
    "    fc_w.append(toy_net_trainer.evaluate(ops=toy_data_net.fc_w))\n",
    "    bias.append(toy_net_trainer.evaluate(ops=toy_data_net.bias))\n",
    "    scale.append(toy_net_trainer.evaluate(ops=toy_data_net.scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vali_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "toy_net_trainer.restore_best()\n",
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "w_post_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "scaler = toy_net_trainer.evaluate(ops=toy_data_net.scaler)\n",
    "\n",
    "plt.plot(-1*w_pre, label='ker_pre_trained')\n",
    "\n",
    "plt.plot(-1*w_post, label='ker_post_trained')\n",
    "plt.plot(-1*w_post_post, label='ker_post_post_trained')\n",
    "plt.legend()\n",
    "print([fc_w, bias, scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test effect of smoothed kernel\n",
    "w_pre_sm = w_pre\n",
    "w_post_sm = w_post \n",
    "w_post_post_sm = w_post_post\n",
    "w_pre_sm[:51] = data_aug_gp.smooth(w_pre[:51], width=10)\n",
    "w_post_sm[:49] = data_aug_gp.smooth(w_post[:49], width=10)\n",
    "w_post_post_sm[:100] = data_aug_gp.smooth(w_post_post[:100], width=5)\n",
    "\n",
    "plt.plot(-1 * w_pre_sm)\n",
    "plt.plot(-1 * w_post_sm)\n",
    "plt.plot(-1*w_post_post_sm)\n",
    "\n",
    "ker_test.kernel_pre = w_pre_sm\n",
    "ker_test.kernel_post = w_post_sm\n",
    "ker_test.kernel_post_post= w_post_post_sm\n",
    "ker_test.kernel_scale = fc_w\n",
    "ker_test.bias = bias\n",
    "ker_test.scale = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compare the target and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Updated the kernel as trained kernel\n",
    "ker_test.kernel_pre = w_pre\n",
    "ker_test.kernel_post = w_post\n",
    "ker_test.kernel_post_post= w_post_post\n",
    "ker_test.kernel_scale = fc_w\n",
    "ker_test.bias = bias\n",
    "ker_test.scale = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate training predictions\n",
    "spk_len = int(data['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "_, predictions_train = arb_w_gen(spk_pairs=spk_pairs_train, spk_len=spk_len, kernel=ker_test, net_type='triplet')\n",
    "\n",
    "ptl_len = np.array([160,3,160,3])\n",
    "rep_time = np.array([1,1,1,1])\n",
    "ptl_len, targets_out, predictions_out = data_aug_knn.target_pred_gen(targets_train, predictions_train, ptl_len, rep_time)\n",
    "\n",
    "# Calculate the total sum of squares\n",
    "R2 = 1 - np.sum(np.square(predictions_train-targets_train))/(np.square(np.std(targets_train))*(len(targets_train)-1))\n",
    "\n",
    "plt.plot(np.linspace(-30,50,80), np.linspace(-30,50,80),'k--')\n",
    "ptl_type = ['stdp', 'trip1', 'quad','trip2']\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(targets_out[i], predictions_out[i],'o', label=ptl_type[i])\n",
    "\n",
    "plt.title('Training data (R2=%0.2f)'%(R2))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate validation predictions\n",
    "spk_len = int(data['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "_, predictions_vali = arb_w_gen(spk_pairs=spk_pairs_vali, spk_len=spk_len, kernel=ker_test, net_type='triplet')\n",
    "\n",
    "ptl_len = np.array([40,1,40,1])\n",
    "rep_time = np.array([1,1,1,1])\n",
    "ptl_len, targets_out, predictions_out = data_aug_knn.target_pred_gen(targets_vali, predictions_vali, ptl_len, rep_time)\n",
    "\n",
    "# Calculate the total sum of squares\n",
    "R2 = 1 - np.sum(np.square(predictions_vali-targets_vali))/(np.square(np.std(targets_vali))*(len(targets_vali)-1))\n",
    "corr = np.sqrt(R2)\n",
    "\n",
    "plt.plot(np.linspace(-30,50,80), np.linspace(-30,50,80),'k--')\n",
    "ptl_type = ['stdp', 'trip1', 'quad','trip2']\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(targets_out[i], predictions_out[i],'o', label=ptl_type[i])\n",
    "\n",
    "plt.title('Validation data (R2=%0.2f)'%(R2))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for STDP\n",
    "data3 = data[data['ptl_idx']==3]\n",
    "ptl_list = [1,2,3,4]\n",
    "spk_len = int(data3['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 0\n",
    "aug_times = [1,1,1,1]\n",
    "spk_pairs_test, targets_test = arb_w_gen(df=data_gen_test, ptl_list=ptl_list, targets=y_test, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate test predictions\n",
    "spk_len = int(data['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "_, predictions_test = arb_w_gen(spk_pairs=spk_pairs_test, spk_len=spk_len, kernel=ker_test, net_type='triplet')\n",
    "\n",
    "ptl_len = np.array([12,7,12,4])\n",
    "rep_time = np.array([1,1,1,1])\n",
    "ptl_len, targets_out, predictions_out = data_aug_knn.target_pred_gen(targets_test, predictions_test, ptl_len, rep_time)\n",
    "\n",
    "# Calculate the total sum of squares\n",
    "R2 = 1 - np.sum(np.square(predictions_test-targets_test))/(np.square(np.std(targets_test))*(len(targets_test)-1))\n",
    "corr = np.sqrt(R2)\n",
    "\n",
    "plt.plot(np.linspace(-30,50,80), np.linspace(-30,50,80),'k--')\n",
    "ptl_type = ['stdp', 'trip1', 'quad','trip2']\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(targets_out[i], predictions_out[i],'o', label=ptl_type[i])\n",
    "\n",
    "plt.title('Testing data (R2=%0.2f)'%(R2))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
