{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split the protocol into training, validation and testing set and then augment and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from modelval import pairptl, network, trainer, dataset, data_aug_knn, perform_eval\n",
    "from modelval.ArbDataGen import arb_w_gen\n",
    "from modelval.spk_visu import spk_see, raster\n",
    "from modelval import gp_regressor\n",
    "from modelval import data_aug_gp\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load data frame\n",
    "data = pd.read_csv('/src/Plasticity_Ker/data/kernel_training_data_auto.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Obtain augmented data from STDP protocol\n",
    "params = {'bias': 7.8571428571428577,\n",
    " 'sigma_kernel': 0.54999999999999993,\n",
    " 'sigma_obs': 0.90000000000000002,\n",
    " 'if_stat_kernel': False\n",
    "         }\n",
    "x_stdp, f_stdp, x_stdp_test, y_stdp_test = data_aug_gp.stdp_gp(**params)\n",
    "\n",
    "x_stdp_train, x_stdp_vali, y_stdp_train, y_stdp_vali = train_test_split(x_stdp, f_stdp, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_stdp_train, y_stdp_train, 'o',label='train_data')\n",
    "plt.plot(x_stdp_vali, y_stdp_vali, 'o',label='vali_data')\n",
    "plt.plot(x_stdp_test,y_stdp_test, 'o', label='test_data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pute time information into dataframe\n",
    "data_stdp_train = data_aug_gp.STDP_dw_gen(x_stdp_train)\n",
    "data_stdp_vali = data_aug_gp.STDP_dw_gen(x_stdp_vali)\n",
    "data_stdp_test = data_aug_gp.STDP_dw_gen(x_stdp_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_stdp_train.shape, y_stdp_train.shape,data_stdp_vali.shape, y_stdp_vali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Generate data for STDP\n",
    "# data3 = data[data['ptl_idx']==3]\n",
    "\n",
    "# # Split into training and testing set (80%, 20%)\n",
    "# # Create train/vali and test data frame\n",
    "# np.random.seed(0)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(data3['dt2'], data3['dw_mean'],test_size=0.2, random_state=0)\n",
    "# plt.plot(x_train, y_train,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x_train_quad, x_test_quad, y_train_quad, y_test_quad = train_test_split(data3['dt2'].values, data3['dw_mean'].values, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x_r = x_train_quad[np.where(x_train_quad>0)[0]].reshape(-1,1)\n",
    "# y_r = y_train_quad[np.where(x_train_quad>0)[0]].reshape(-1,1)\n",
    "# x_test_r = np.linspace(np.min(x_r),120,120).reshape(-1,1)\n",
    "\n",
    "# x_l = x_train_quad[np.where(x_train_quad<0)[0]].reshape(-1,1)\n",
    "# y_l = y_train_quad[np.where(x_train_quad<0)[0]].reshape(-1,1)\n",
    "# x_test_l = np.linspace(-120,np.max(x_l),120).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# gp_rg = gp_regressor.GP_regressor(x_r, y_r, x_test_r,sigma_kernel=1.9, scale=5, bias=2.78, sigma_obs=3.0, noise_const=96.7, if_stat_kernel=False, if_stat_noise=False)\n",
    "# f_r, v_f_r, lp = gp_rg.fit()\n",
    "# std = np.sqrt(v_f_r.transpose().diagonal()).reshape(-1,1)\n",
    "# plt.plot(x_r, y_r,'o', label='Raw_data_train')\n",
    "# plt_range1 = np.arange(0,112,1)\n",
    "\n",
    "# plt.plot(x_test_r, f_r,'g')\n",
    "# plt.fill_between(np.squeeze(x_test_r), np.squeeze(f_r-1.96*std), np.squeeze(f_r+1.96*std), alpha=1, color='deeppink')\n",
    "\n",
    "# gp_rg = gp_regressor.GP_regressor(x_l, y_l, x_test_l,sigma_kernel=1.9, scale=5, bias=2.78, sigma_obs=3.0, noise_const=96.7, if_stat_kernel=False, if_stat_noise=False)\n",
    "# f_l, v_f_l, lp = gp_rg.fit()\n",
    "# std = np.sqrt(v_f_l.transpose().diagonal()).reshape(-1,1)\n",
    "# plt.plot(x_l, y_l,'o', label='Raw_data_train')\n",
    "# plt.plot(x_test_l, f_l,'g')\n",
    "# plt.fill_between(np.squeeze(x_test_l), np.squeeze(f_l-1.96*std), np.squeeze(f_l+1.96*std), alpha=1, color='deeppink')\n",
    "# #plt.fill_between(np.squeeze(x_aug), np.squeeze(f-1.96*std), np.squeeze(f+1.96*std), alpha=1, color='deeppink', label=\"95% confidence interval\")\n",
    "\n",
    "# # Sample from the gp regression\n",
    "# for i in range(len(f_l)):\n",
    "#     np.random.seed(i)\n",
    "#     scale = 5 * np.exp(-1 * np.abs(x_test_l[i])/96.7)\n",
    "#     noise = np.random.normal(loc=0, scale=scale, size=1)\n",
    "#     f_l[i] = f_l[i] + noise\n",
    "\n",
    "# for i in range(len(f_r)):\n",
    "#     np.random.seed(i)\n",
    "#     scale = 5 * np.exp(-1 * np.abs(x_test_r[i])/96.7)\n",
    "#     noise = np.random.normal(loc=0, scale=scale, size=1)\n",
    "#     f_r[i] = f_r[i] + noise\n",
    "\n",
    "# plt.plot(x_test_r, f_r, 'ro', label='Sampled data')\n",
    "# plt.plot(x_test_l, f_l, 'ro', label='Sampled data')\n",
    "\n",
    "# plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Obtain augmented data from quadruplet protocol\n",
    "# Obtain augmented data from STDP protocol\n",
    "params = {'bias': 6.4285714285714288,\n",
    " 'sigma_kernel': 0.84999999999999998,\n",
    " 'sigma_obs': 0.79999999999999993,\n",
    " 'if_stat_kernel': False}\n",
    "\n",
    "x_train_r, y_train_r, x_train_l, y_train_l, x_quad_test, y_quad_test =data_aug_gp.quad_gp(**params)\n",
    "# Split into training and validation dataset\n",
    "x_quad = np.concatenate([x_train_l,x_train_r])\n",
    "y_quad = np.concatenate([y_train_l,y_train_r])\n",
    "\n",
    "# Split into training and validation set\n",
    "x_quad_train, x_quad_vali, y_quad_train, y_quad_vali = train_test_split(x_quad, y_quad, test_size=0.2)\n",
    "\n",
    "plt.plot(x_quad_train, y_quad_train, 'o', label='train_data')\n",
    "plt.plot(x_quad_vali, y_quad_vali, 'o', label='vali_data')\n",
    "plt.plot(x_quad_test, y_quad_test, 'o', label='test_data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Put dt information into dataframe\n",
    "data_quad_train = data_aug_gp.quad_dw_gen(x_quad_train)\n",
    "data_quad_vali = data_aug_gp.quad_dw_gen(x_quad_vali)\n",
    "data_quad_test = data_aug_gp.quad_dw_gen(x_quad_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_quad_train.shape, y_quad_train.shape, data_quad_vali.shape, y_quad_vali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Combine data from stdp and quadruplet\n",
    "data_gen_train = pd.concat([data_stdp_train, data_quad_train], axis=0)\n",
    "y_train = np.concatenate([y_stdp_train, y_quad_train])\n",
    "data_gen_vali = pd.concat([data_stdp_vali, data_quad_vali], axis=0)\n",
    "y_vali = np.concatenate([y_stdp_vali, y_quad_vali])\n",
    "data_gen_test = pd.concat([data_stdp_test, data_quad_test], axis=0)\n",
    "y_test = np.concatenate([y_stdp_test, y_quad_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_train.shape, y_train.shape, data_gen_vali.shape, y_vali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trip_para = pd.read_pickle('/src/Plasticity_Ker/data/Gerstner_trip_para_df')\n",
    "trip_para\n",
    "# Reorder columns to match parameter of the model\n",
    "trip_para = trip_para[['A2_+', 'A3_-', 'A2_-', 'A3_+', 'Tau_+', 'Tau_x', 'Tau_-', 'Tau_y']]\n",
    "trip_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Visualize kernel\n",
    "from modelval.kernel import KernelGen\n",
    "ker_test = KernelGen()\n",
    "\n",
    "para = trip_para.loc[('Hippo_AlltoAll', 'Full'), :]\n",
    "a = para[:4].values\n",
    "tau = para[4:].values\n",
    "reso_set = 2\n",
    "tau_pre_post = tau[0]/reso_set  # ms\n",
    "tau_post_pre = tau[2]/reso_set # ms\n",
    "\n",
    "ker_test = KernelGen(len_kernel=101)\n",
    "ker_test.trip_model_ker(para, data_name='Hippocampus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for STDP\n",
    "data3 = data[data['ptl_idx']==3]\n",
    "ptl_list = [1,3]\n",
    "spk_len = int(data3['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 0\n",
    "aug_times = [1,1]\n",
    "spk_pairs_train, targets_train = arb_w_gen(df=data_gen_train, ptl_list=ptl_list, targets=y_train, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)\n",
    "spk_pairs_vali, targets_vali = arb_w_gen(df=data_gen_vali, ptl_list=ptl_list, targets=y_vali, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the network\n",
    "ground_truth_init = 0\n",
    "reg_scale=(1, 1)\n",
    "toy_data_net = network.TripNet(kernel=ker_test, ground_truth_init=ground_truth_init, reg_scale=reg_scale, n_input=spk_pairs_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "save_dir= '/src/Plasticity_Ker/model/Pair_ptl1_3_real_aug_gp_no_noise'\n",
    "# optimizer_op = tf.train.GradientDescentOptimizer\n",
    "toy_net_trainer = trainer.Trainer(toy_data_net.loss, toy_data_net.loss, input_name=toy_data_net.inputs, target_name=toy_data_net.target, save_dir=save_dir, optimizer_config={'learning_rate': toy_data_net.lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = dataset.Dataset(spk_pairs_train, targets_train)\n",
    "vali_data = dataset.Dataset(spk_pairs_vali, targets_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "w_post_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "plt.plot(w_pre,  label='ker_pre_init')\n",
    "plt.plot(w_post,  label='ker_post_init')\n",
    "plt.plot(w_post_post,  label='ker_post_post_init')\n",
    "\n",
    "plt.legend()\n",
    "print(fc_w, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Learn the kernel from random initialization\n",
    "learning_rate = 0.001\n",
    "iterations = 5\n",
    "min_error = -1\n",
    "for i in range(iterations):\n",
    "    toy_net_trainer.train(train_data, vali_data, batch_size=128, min_error=min_error, feed_dict={toy_data_net.lr: learning_rate})\n",
    "    learning_rate = learning_rate/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Restore the model and parameter\n",
    "toy_net_trainer.restore_best()\n",
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "w_post_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "plt.plot(-1 * w_pre,label='ker_pre_trained')\n",
    "plt.plot(-1 * w_post,label='ker_post_trained')\n",
    "plt.plot(w_post_post,label='ker_post_post_trained')\n",
    "\n",
    "plt.legend()\n",
    "print(fc_w, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Updated the kernel as trained kernel\n",
    "ker_test.kernel_pre = w_pre\n",
    "ker_test.kernel_post = w_post\n",
    "ker_test.kernel_post_post= w_post_post\n",
    "ker_test.kernel_scale = fc_w\n",
    "ker_test.bias = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate training predictions\n",
    "spk_len = int(data['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "_, predictions = arb_w_gen(spk_pairs=spk_pairs_train, spk_len=spk_len, kernel=ker_test, net_type='triplet')\n",
    "plt.plot(targets_train[:160,:], predictions[:160,:], 'o')\n",
    "plt.plot(targets_train[160:,:], predictions[160:,:], 'o')\n",
    "\n",
    "plt.plot(np.linspace(-30,60,90),np.linspace(-30,60,90),'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate validation predictions\n",
    "spk_len = int(data['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_vali, spk_len=spk_len, kernel=ker_test, net_type='triplet')\n",
    "plt.plot(targets_vali, predictions, 'o')\n",
    "plt.plot(np.linspace(-30,60,90),np.linspace(-30,60,90),'--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Evaluate the test restult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for STDP\n",
    "data3 = data[data['ptl_idx']==3]\n",
    "ptl_list = [1,3]\n",
    "spk_len = int(data3['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 0\n",
    "aug_times = [1,1]\n",
    "spk_pairs_test, targets_test = arb_w_gen(df=data_gen_test, ptl_list=ptl_list, targets=y_test, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate validation predictions\n",
    "spk_len = int(data['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_test, spk_len=spk_len, kernel=ker_test, net_type='triplet')\n",
    "plt.plot(targets_test, predictions, 'o')\n",
    "plt.plot(np.linspace(-30,60,90),np.linspace(-30,60,90),'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Generate data for Triplet\n",
    "# dt = np.array([-10, -5, 0, 5, 10]).reshape(-1,1)\n",
    "# data2_gen, targets2 = dw_gen.triplet_dw_gen(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Visualize the triplet data\n",
    "# data2_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Generate data for Quadruplet\n",
    "# data3 = data[data['ptl_idx']==3]\n",
    "# data3_gen, targets3 = dw_gen.quad_dw_gen(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# width_list = np.concatenate([np.linspace(10,3,45), np.linspace(3,10,45)])\n",
    "# plt.plot(data3['dt2'], data3['dw_mean'],'o', label='Raw data')\n",
    "# plt.plot(data3_gen['dt2'], targets3,'o', label='KNN')\n",
    "# targets3_sm = np.concatenate([dw_gen.smooth(targets3[:45],width_list = width_list), dw_gen.smooth(targets3[45:],width_list = width_list)])\n",
    "# plt.plot(data3_gen['dt2'],targets3_sm,'o', label='Smoothness filter')\n",
    "# plt.xlabel('dt(ms)')\n",
    "# plt.ylabel('$\\Delta w$')\n",
    "# plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Sample randomly the smoothed Quadruplet data\n",
    "# samp_len = len(targets3_sm)\n",
    "# np.random.seed(1)\n",
    "# test_idx_quad = np.unique(np.random.randint(low=0, high=90, size=9))\n",
    "# train_vali_idx = np.setdiff1d(np.linspace(0,89,90), test_idx_quad).astype(int)\n",
    "# np.random.seed(10)\n",
    "# vali_idx_idx = np.random.randint(low=0, high=80, size=18)\n",
    "# vali_idx_quad = np.unique(train_vali_idx[vali_idx_idx])\n",
    "# train_idx_quad = np.setdiff1d(train_vali_idx, vali_idx_quad).astype(int)\n",
    "# plt.plot(data3_gen.loc[train_idx_quad]['dt2'],targets3_sm[train_idx_quad],'o', label='train_data')\n",
    "# plt.plot(data3_gen.loc[vali_idx_quad]['dt2'],targets3_sm[vali_idx_quad],'o', label='vali_data')\n",
    "# plt.plot(data3_gen.loc[test_idx_quad]['dt2'],targets3_sm[test_idx_quad],'o', label='test_data')\n",
    "# plt.legend()\n",
    "# print(len(set(train_idx_quad)), len(set(vali_idx_quad)), len(set(test_idx_quad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Visualize kernel\n",
    "from modelval.kernel import KernelGen\n",
    "ker_test = KernelGen()\n",
    "\n",
    "para = trip_para.loc[('Hippo_AlltoAll', 'Full'), :]\n",
    "a = para[:4].values\n",
    "tau = para[4:].values\n",
    "reso_set = 2\n",
    "tau_pre_post = tau[0]/reso_set  # ms\n",
    "tau_post_pre = tau[2]/reso_set # ms\n",
    "\n",
    "ker_test = KernelGen(len_kernel=101)\n",
    "ker_test.trip_model_ker(para, data_name='Hippocampus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_train = pd.concat([data1_gen.loc[train_idx_stdp],data2_gen, data3_gen.loc[train_idx_quad]])\n",
    "targets_gen_train = np.concatenate([targets1_sm[train_idx_stdp], targets2, targets3_sm[train_idx_quad]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_vali = pd.concat([data1_gen.loc[vali_idx_stdp], data2_gen, data3_gen.loc[vali_idx_quad]])\n",
    "targets_gen_vali = np.concatenate([targets1_sm[vali_idx_stdp], targets2, targets3_sm[vali_idx_quad]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_test = pd.concat([data1_gen.loc[test_idx_stdp], data2_gen, data3_gen.loc[test_idx_quad]])\n",
    "targets_gen_test = np.concatenate([targets1_sm[test_idx_stdp], targets2, targets3_sm[test_idx_quad]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# len_stdp = len(vali_idx_stdp)*\n",
    "# len_triplet = len(vali_idx_stdp)*20+len(data2_gen[data2_gen['ptl_idx']==2])*40\n",
    "# len_trip2 = len(vali_idx_stdp)*20+len(data2_gen[data2_gen['ptl_idx']==2])*40+len(data2_gen[data2_gen['ptl_idx']==4])*40\n",
    "# len_quad = len(targets_gen_vali) - len_trip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_train['ptl_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for STDP\n",
    "ptl_list = [1,2,4,3]\n",
    "spk_len = int(data3_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 1\n",
    "aug_times = [20,40,40,20]\n",
    "spk_pairs_train, targets_train = arb_w_gen(df=data_gen_train, ptl_list=ptl_list, targets=targets_gen_train, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for Quadruplet\n",
    "ptl_list = [1,2,4,3]\n",
    "spk_len = int(data3_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 1\n",
    "aug_times = [20,40,40,20]\n",
    "spk_pairs_vali, targets_vali = arb_w_gen(df=data_gen_vali, ptl_list=ptl_list, targets=targets_gen_vali, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spk_pairs_train.shape, spk_pairs_vali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the network\n",
    "ground_truth_init = 0\n",
    "reg_scale=(1, 1)\n",
    "init_seed=(4,5,6,7)\n",
    "toy_data_net = network.TripNet(kernel=ker_test, ground_truth_init=ground_truth_init, init_seed=init_seed, reg_scale=reg_scale, n_input=spk_pairs_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "save_dir= '/src/Plasticity_Ker/model/Trip_ptl1_4_real_aug'\n",
    "# optimizer_op = tf.train.GradientDescentOptimizer\n",
    "toy_net_trainer = trainer.Trainer(toy_data_net.loss, toy_data_net.loss, input_name=toy_data_net.inputs, optimizer_op=optimizer_op, target_name=toy_data_net.target, save_dir=save_dir, optimizer_config={'learning_rate': toy_data_net.lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = dataset.Dataset(spk_pairs_train, targets_train)\n",
    "vali_data = dataset.Dataset(spk_pairs_vali, targets_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "w_post_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "plt.plot(w_pre,  label='ker_pre_trained')\n",
    "plt.plot(w_post,  label='ker_post_trained')\n",
    "plt.plot(w_post_post,  label='ker_post_trained')\n",
    "plt.legend()\n",
    "print(fc_w, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Learn the kernel from random initialization\n",
    "learning_rate = 0.001\n",
    "iterations = 5\n",
    "min_error = -1\n",
    "for i in range(iterations):\n",
    "    toy_net_trainer.train(train_data, vali_data, batch_size=128, min_error=min_error, feed_dict={toy_data_net.lr: learning_rate})\n",
    "    learning_rate = learning_rate/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "toy_net_trainer.restore_best()\n",
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "w_post_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "plt.plot(-1*w_pre, label='ker_pre_trained')\n",
    "plt.plot(-1*w_post, label='ker_post_trained')\n",
    "plt.plot(-1*w_post_post, label='ker_post_post_trained')\n",
    "plt.legend()\n",
    "print([fc_w, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Test effect of smoothed kernel\n",
    "# w_pre_sm = w_pre\n",
    "# w_post_sm = w_post \n",
    "# w_post_post_sm = w_post_post\n",
    "# w_pre_sm[:50] = dw_gen.smooth(w_pre[:50], width=2)\n",
    "# w_post_sm[:48] = dw_gen.smooth(w_post[:48], width=2)\n",
    "# w_post_post_sm[:49] = dw_gen.smooth(w_post_post[:49], width=3)\n",
    "\n",
    "# plt.plot(-1 * w_pre_sm)\n",
    "# plt.plot(-1 * w_post_sm)\n",
    "# plt.plot(-1*w_post_post_sm)\n",
    "\n",
    "# ker_test.kernel_pre = w_pre_sm\n",
    "# ker_test.kernel_post = w_post_sm\n",
    "# ker_test.kernel_post_post= w_post_post_sm\n",
    "# ker_test.kernel_scale = fc_w\n",
    "# ker_test.bias = bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compare the target and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ker_test.kernel_pre = w_pre\n",
    "ker_test.kernel_post = w_post\n",
    "# ker_test.kernel_post_post= w_post_post\n",
    "ker_test.kernel_scale = fc_w\n",
    "ker_test.bias = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate training predictions\n",
    "spk_len = int(data1_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_train, spk_len=spk_len, kernel=ker_test, net_type='triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_len = [len(train_idx_stdp), len(data2_gen[data2_gen['ptl_idx']==2]), len(data2_gen[data2_gen['ptl_idx']==4]), len(train_idx_quad)]\n",
    "ptl_whole_len, targets_ptl, predictions_ptl = dw_gen.target_pred_gen(targets_train, predictions, ptl_len, [20, 40, 40, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_name = ['stdp', 'triplet', 'trip2', 'quadruplet']\n",
    "x_fit = np.linspace(np.min(targets_vali)-1, np.max(targets_vali)+1, 100)\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(np.linspace(-30,70,100),np.linspace(-30,70,100), 'k--' )\n",
    "    # R2, corr, y_fit = perform_eval.R2_corr(predictions_ptl[i],targets_ptl[i], x_fit)\n",
    "    plt.plot(targets_ptl[i], predictions_ptl[i], 'o', label=ptl_name[i]+'(n={a})'.format(a=targets_ptl[i].shape[0]))\n",
    "\n",
    "R2, corr, y_fit = perform_eval.R2_corr(predictions,targets_train, x_fit)    \n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.title(''.join(('R2=%.4f'%(R2), ', Corr=%.4f'%(corr))))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for Quadruplet\n",
    "ptl_list = [1,2,4,3]\n",
    "spk_len = int(data3_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 1\n",
    "aug_times = [1,1,1,1]\n",
    "spk_pairs_vali, targets_vali = arb_w_gen(df=data_gen_vali, ptl_list=ptl_list, targets=targets_gen_vali, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate validation predictions\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_vali, spk_len=spk_len, kernel=ker_test, net_type='triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_len = [len(vali_idx_stdp), len(data2_gen[data2_gen['ptl_idx']==2]), len(data2_gen[data2_gen['ptl_idx']==4]), len(vali_idx_quad)]\n",
    "ptl_whole_len, targets_ptl, predictions_ptl = dw_gen.target_pred_gen(targets_vali, predictions, ptl_len, aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_name = ['stdp', 'triplet', 'trip2', 'quadruplet']\n",
    "x_fit = np.linspace(np.min(targets_vali)-1, np.max(targets_vali)+1, 100)\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(np.linspace(-30,70,100),np.linspace(-30,70,100), 'k--' )\n",
    "    # R2, corr, y_fit = perform_eval.R2_corr(predictions_ptl[i],targets_ptl[i], x_fit)\n",
    "    plt.plot(targets_ptl[i], predictions_ptl[i], 'o', label=ptl_name[i]+'(n={a})'.format(a=targets_ptl[i].shape[0]))\n",
    "\n",
    "R2, corr, y_fit = perform_eval.R2_corr(predictions,targets_vali, x_fit)    \n",
    "\n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.title(''.join(('R2=%.4f'%(R2), ', Corr=%.4f'%(corr))))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for Quadruplet\n",
    "ptl_list = [1,2,4,3]\n",
    "spk_len = int(data3_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 0\n",
    "aug_times = [1,1,1,1]\n",
    "spk_pairs_test, targets_test = arb_w_gen(df=data_gen_test, targets=targets_gen_test, ptl_list=ptl_list, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)\n",
    "test= spk_pairs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate test restuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate validation predictions\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_test, spk_len=spk_len, kernel=ker_test, net_type='triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_len = [len(test_idx_stdp), len(data2_gen[data2_gen['ptl_idx']==2]), len(data2_gen[data2_gen['ptl_idx']==4]), len(test_idx_quad)]\n",
    "ptl_whole_len, targets_ptl, predictions_ptl = dw_gen.target_pred_gen(targets_test, predictions, ptl_len, aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_name = ['Stdp', 'Triplet', 'Trip2', 'Quadruplet']\n",
    "x_fit = np.linspace(np.min(targets_vali)-1, np.max(targets_vali)+1, 100)\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(np.linspace(-30,70,100),np.linspace(-30,70,100), 'k--' )\n",
    "    # R2, corr, y_fit = perform_eval.R2_corr(predictions_ptl[i],targets_ptl[i], x_fit)\n",
    "    plt.plot(targets_ptl[i], predictions_ptl[i], 'o', label=ptl_name[i]+'(n={a})'.format(a=targets_ptl[i].shape[0]))\n",
    "\n",
    "R2, corr, y_fit = perform_eval.R2_corr(predictions,targets_test, x_fit)    \n",
    "\n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.title(''.join(('R2=%.4f'%(R2), ', Corr=%.4f'%(corr))))\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
