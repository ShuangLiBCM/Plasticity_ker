{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split the protocol into training, validation and testing set and then augment and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from modelval import pairptl, network, trainer, dataset, dw_gen, perform_eval\n",
    "from modelval.ArbDataGen import arb_w_gen\n",
    "from modelval.spk_visu import spk_see, raster\n",
    "from modelval import gp_regressor\n",
    "from modelval import data_aug_gp\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptl_idx</th>\n",
       "      <th>pre_spk_num</th>\n",
       "      <th>pre_spk_freq</th>\n",
       "      <th>post_spk_num</th>\n",
       "      <th>post_spk_freq</th>\n",
       "      <th>ptl_occ</th>\n",
       "      <th>ptl_freq</th>\n",
       "      <th>dt1</th>\n",
       "      <th>dt2</th>\n",
       "      <th>dt3</th>\n",
       "      <th>dw_mean</th>\n",
       "      <th>dw_ste</th>\n",
       "      <th>train_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-102.898046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.322590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-75.579896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.674768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-80.871473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-12.696449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-67.562239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.231446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-63.553410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.990216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ptl_idx  pre_spk_num  pre_spk_freq  post_spk_num  post_spk_freq  ptl_occ  \\\n",
       "0        1            1             0             1              0       60   \n",
       "1        1            1             0             1              0       60   \n",
       "2        1            1             0             1              0       60   \n",
       "3        1            1             0             1              0       60   \n",
       "4        1            1             0             1              0       60   \n",
       "\n",
       "   ptl_freq         dt1  dt2  dt3    dw_mean  dw_ste  train_len  \n",
       "0       1.0 -102.898046  0.0    0  -1.322590     0.0         60  \n",
       "1       1.0  -75.579896  0.0    0   2.674768     0.0         60  \n",
       "2       1.0  -80.871473  0.0    0 -12.696449     0.0         60  \n",
       "3       1.0  -67.562239  0.0    0   0.231446     0.0         60  \n",
       "4       1.0  -63.553410  0.0    0  -0.990216     0.0         60  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data frame\n",
    "data = pd.read_csv('/src/Plasticity_Ker/data/kernel_training_data_auto.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Obtain augmented data from STDP protocol\n",
    "x_aug, f_samp, x_test, y_test = data_aug_gp.stdp_gp()\n",
    "\n",
    "# Split into training and validation dataset\n",
    "X_train_stdp, X_vali_stdp, y_train_stdp, y_vali_stdp = train_test_split(x_aug, f_samp, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pute time information into dataframe\n",
    "data_stdp_train = data_aug_gp.STDP_dw_gen(X_train_stdp)\n",
    "data_stdp_vali = data_aug_gp.STDP_dw_gen(X_vali_stdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160, 13), (160, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stdp_train.shape, y_train_stdp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Generate data for STDP\n",
    "# data3 = data[data['ptl_idx']==3]\n",
    "\n",
    "# # Split into training and testing set (80%, 20%)\n",
    "# # Create train/vali and test data frame\n",
    "# np.random.seed(0)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(data3['dt2'], data3['dw_mean'],test_size=0.2, random_state=0)\n",
    "# plt.plot(x_train, y_train,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x_train_quad, x_test_quad, y_train_quad, y_test_quad = train_test_split(data3['dt2'].values, data3['dw_mean'].values, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x_r = x_train_quad[np.where(x_train_quad>0)[0]].reshape(-1,1)\n",
    "# y_r = y_train_quad[np.where(x_train_quad>0)[0]].reshape(-1,1)\n",
    "# x_test_r = np.linspace(np.min(x_r),120,120).reshape(-1,1)\n",
    "\n",
    "# x_l = x_train_quad[np.where(x_train_quad<0)[0]].reshape(-1,1)\n",
    "# y_l = y_train_quad[np.where(x_train_quad<0)[0]].reshape(-1,1)\n",
    "# x_test_l = np.linspace(-120,np.max(x_l),120).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# gp_rg = gp_regressor.GP_regressor(x_r, y_r, x_test_r,sigma_kernel=1.9, scale=5, bias=2.78, sigma_obs=3.0, noise_const=96.7, if_stat_kernel=False, if_stat_noise=False)\n",
    "# f_r, v_f_r, lp = gp_rg.fit()\n",
    "# std = np.sqrt(v_f_r.transpose().diagonal()).reshape(-1,1)\n",
    "# plt.plot(x_r, y_r,'o', label='Raw_data_train')\n",
    "# plt_range1 = np.arange(0,112,1)\n",
    "\n",
    "# plt.plot(x_test_r, f_r,'g')\n",
    "# plt.fill_between(np.squeeze(x_test_r), np.squeeze(f_r-1.96*std), np.squeeze(f_r+1.96*std), alpha=1, color='deeppink')\n",
    "\n",
    "# gp_rg = gp_regressor.GP_regressor(x_l, y_l, x_test_l,sigma_kernel=1.9, scale=5, bias=2.78, sigma_obs=3.0, noise_const=96.7, if_stat_kernel=False, if_stat_noise=False)\n",
    "# f_l, v_f_l, lp = gp_rg.fit()\n",
    "# std = np.sqrt(v_f_l.transpose().diagonal()).reshape(-1,1)\n",
    "# plt.plot(x_l, y_l,'o', label='Raw_data_train')\n",
    "# plt.plot(x_test_l, f_l,'g')\n",
    "# plt.fill_between(np.squeeze(x_test_l), np.squeeze(f_l-1.96*std), np.squeeze(f_l+1.96*std), alpha=1, color='deeppink')\n",
    "# #plt.fill_between(np.squeeze(x_aug), np.squeeze(f-1.96*std), np.squeeze(f+1.96*std), alpha=1, color='deeppink', label=\"95% confidence interval\")\n",
    "\n",
    "# # Sample from the gp regression\n",
    "# for i in range(len(f_l)):\n",
    "#     np.random.seed(i)\n",
    "#     scale = 5 * np.exp(-1 * np.abs(x_test_l[i])/96.7)\n",
    "#     noise = np.random.normal(loc=0, scale=scale, size=1)\n",
    "#     f_l[i] = f_l[i] + noise\n",
    "\n",
    "# for i in range(len(f_r)):\n",
    "#     np.random.seed(i)\n",
    "#     scale = 5 * np.exp(-1 * np.abs(x_test_r[i])/96.7)\n",
    "#     noise = np.random.normal(loc=0, scale=scale, size=1)\n",
    "#     f_r[i] = f_r[i] + noise\n",
    "    \n",
    "# plt.plot(x_test_r, f_r, 'ro', label='Sampled data')\n",
    "# plt.plot(x_test_l, f_l, 'ro', label='Sampled data')\n",
    "\n",
    "\n",
    "# plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Obtain augmented data from quadruplet protocol\n",
    "x_test_r, f_r_samp, x_test_l, f_l_samp, x_test, y_test =data_aug_gp.quad_gp()\n",
    "\n",
    "# Split into training and validation dataset\n",
    "X_train_quad_l, X_vali_quad_l, y_train_quad_l, y_vali_quad_l = train_test_split(x_test_l, f_l_samp, test_size=0.2)\n",
    "X_train_quad_r, X_vali_quad_r, y_train_quad_r, y_vali_quad_r = train_test_split(x_test_r, f_r_samp, test_size=0.2)\n",
    "X_train_quad = np.concatenate([X_train_quad_l,X_train_quad_r])\n",
    "y_train_quad = np.concatenate([y_train_quad_l,y_train_quad_r])\n",
    "X_vali_quad = np.concatenate([X_vali_quad_l,X_vali_quad_r])\n",
    "y_vali_quad = np.concatenate([y_vali_quad_l,y_vali_quad_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Put dt information into dataframe\n",
    "data_quad_train = data_aug_gp.quad_dw_gen(X_train_quad)\n",
    "data_quad_vali = data_aug_gp.quad_dw_gen(X_vali_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192, 13), (192, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quad_train.shape, y_train_quad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_train = pd.concat([data_stdp_train, data_quad_train], axis=0)\n",
    "y_train = np.concatenate([y_train_stdp, y_train_quad])\n",
    "data_gen_vali = pd.concat([data_stdp_vali, data_quad_vali], axis=0)\n",
    "y_vali = np.concatenate([y_vali_stdp, y_vali_quad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((352, 13), (352, 1), (88, 13), (88, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen_train.shape, y_train.shape, data_gen_vali.shape, y_vali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>A2_+</th>\n",
       "      <th>A3_-</th>\n",
       "      <th>A2_-</th>\n",
       "      <th>A3_+</th>\n",
       "      <th>Tau_+</th>\n",
       "      <th>Tau_x</th>\n",
       "      <th>Tau_-</th>\n",
       "      <th>Tau_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Visu_AlltoAll</th>\n",
       "      <th>Full</th>\n",
       "      <td>5e-10</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>16.8</td>\n",
       "      <td>101</td>\n",
       "      <td>33.7</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1</td>\n",
       "      <td>33.7</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Visu_NearestSpk</th>\n",
       "      <th>Full</th>\n",
       "      <td>8.8e-11</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.053</td>\n",
       "      <td>16.8</td>\n",
       "      <td>714</td>\n",
       "      <td>33.7</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.05</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1</td>\n",
       "      <td>33.7</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Hippo_AlltoAll</th>\n",
       "      <th>Full</th>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>16.8</td>\n",
       "      <td>946</td>\n",
       "      <td>33.7</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>0.0053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.008</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1</td>\n",
       "      <td>33.7</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Hippo_NearestSpk</th>\n",
       "      <th>Full</th>\n",
       "      <td>0.0046</td>\n",
       "      <td>7.5e-09</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>16.8</td>\n",
       "      <td>575</td>\n",
       "      <td>33.7</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>0.0046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1</td>\n",
       "      <td>33.7</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          A2_+     A3_-    A2_-    A3_+ Tau_+ Tau_x Tau_-  \\\n",
       "Visu_AlltoAll    Full    5e-10  0.00023   0.007  0.0062  16.8   101  33.7   \n",
       "                 Min         0        0  0.0071  0.0065  16.8     1  33.7   \n",
       "Visu_NearestSpk  Full  8.8e-11   0.0031  0.0066   0.053  16.8   714  33.7   \n",
       "                 Min         0        0   0.008    0.05  16.8     1  33.7   \n",
       "Hippo_AlltoAll   Full   0.0061   0.0014  0.0016  0.0067  16.8   946  33.7   \n",
       "                 Min    0.0053        0  0.0035   0.008  16.8     1  33.7   \n",
       "Hippo_NearestSpk Full   0.0046  7.5e-09   0.003  0.0091  16.8   575  33.7   \n",
       "                 Min    0.0046        0   0.003  0.0091  16.8     1  33.7   \n",
       "\n",
       "                      Tau_y  \n",
       "Visu_AlltoAll    Full   125  \n",
       "                 Min    114  \n",
       "Visu_NearestSpk  Full    40  \n",
       "                 Min     40  \n",
       "Hippo_AlltoAll   Full    27  \n",
       "                 Min     40  \n",
       "Hippo_NearestSpk Full    47  \n",
       "                 Min     48  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_para = pd.read_pickle('/src/Plasticity_Ker/data/Gerstner_trip_para_df')\n",
    "trip_para\n",
    "# Reorder columns to match parameter of the model\n",
    "trip_para = trip_para[['A2_+', 'A3_-', 'A2_-', 'A3_+', 'Tau_+', 'Tau_x', 'Tau_-', 'Tau_y']]\n",
    "trip_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Visualize kernel\n",
    "from modelval.kernel import KernelGen\n",
    "ker_test = KernelGen()\n",
    "\n",
    "para = trip_para.loc[('Hippo_AlltoAll', 'Full'), :]\n",
    "a = para[:4].values\n",
    "tau = para[4:].values\n",
    "reso_set = 2\n",
    "tau_pre_post = tau[0]/reso_set  # ms\n",
    "tau_post_pre = tau[2]/reso_set # ms\n",
    "\n",
    "ker_test = KernelGen(len_kernel=51)\n",
    "ker_test.trip_model_ker(para, data_name='Hippocampus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for STDP\n",
    "ptl_list = [1,3]\n",
    "spk_len = int(data['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 1\n",
    "aug_times = [10,10]\n",
    "spk_pairs_train, targets_train = arb_w_gen(df=data_gen_train, ptl_list=ptl_list, targets=y_train, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)\n",
    "spk_pairs_vali, targets_vali = arb_w_gen(df=data_gen_vali, ptl_list=ptl_list, targets=y_vali, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the network\n",
    "ground_truth_init = 0\n",
    "reg_scale=(1, 1)\n",
    "toy_data_net = network.TripNet(kernel=ker_test, ground_truth_init=ground_truth_init, reg_scale=reg_scale, n_input=spk_pairs_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "save_dir= '/src/Plasticity_Ker/model/Pair_ptl1_3_real_aug_gp'\n",
    "# optimizer_op = tf.train.GradientDescentOptimizer\n",
    "toy_net_trainer = trainer.Trainer(toy_data_net.loss, toy_data_net.loss, input_name=toy_data_net.inputs, target_name=toy_data_net.target, save_dir=save_dir, optimizer_config={'learning_rate': toy_data_net.lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = dataset.Dataset(spk_pairs_train, targets_train)\n",
    "vali_data = dataset.Dataset(spk_pairs_vali, targets_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "w_post_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "plt.plot(w_pre,  label='ker_pre_init')\n",
    "plt.plot(w_post,  label='ker_post_init')\n",
    "plt.plot(w_post_post,  label='ker_post_post_init')\n",
    "\n",
    "plt.legend()\n",
    "print(fc_w, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Learn the kernel from random initialization\n",
    "learning_rate = 0.001\n",
    "iterations = 5\n",
    "min_error = -1\n",
    "for i in range(iterations):\n",
    "    toy_net_trainer.train(train_data, vali_data, batch_size=128, min_error=min_error, feed_dict={toy_data_net.lr: learning_rate})\n",
    "    learning_rate = learning_rate/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "toy_net_trainer.restore_best()\n",
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "plt.plot(w_pre, label='ker_pre_trained')\n",
    "plt.plot(w_post, label='ker_post_trained')\n",
    "plt.legend()\n",
    "print([fc_w, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ker_test.kernel_pre = w_pre\n",
    "ker_test.kernel_post = w_post\n",
    "# ker_test.kernel_post_post= w_post_post\n",
    "ker_test.kernel_scale = fc_w\n",
    "ker_test.bias = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate training predictions\n",
    "spk_len = int(data1['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_train, spk_len=spk_len, kernel=ker_test, net_type='pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(targets_train, predictions, 'o')\n",
    "plt.plot(np.linspace(-30,60,90),np.linspace(-30,60,90),'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_len = [len(train_idx_stdp)]\n",
    "ptl_whole_len, targets_ptl, predictions_ptl = dw_gen.target_pred_gen(targets_train, predictions, ptl_len, [20, 40, 40, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_name = ['stdp']\n",
    "x_fit = np.linspace(np.min(targets_vali)-1, np.max(targets_vali)+1, 100)\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(np.linspace(-30,70,100),np.linspace(-30,70,100), 'k--' )\n",
    "    # R2, corr, y_fit = perform_eval.R2_corr(predictions_ptl[i],targets_ptl[i], x_fit)\n",
    "    plt.plot(targets_ptl[i], predictions_ptl[i], 'o', label=ptl_name[i]+'(n={a})'.format(a=targets_ptl[i].shape[0]))\n",
    "\n",
    "R2, corr, y_fit = perform_eval.R2_corr(predictions,targets_train, x_fit)    \n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.title(''.join(('R2=%.4f'%(R2), ', Corr=%.4f'%(corr))))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Generate data for Triplet\n",
    "# dt = np.array([-10, -5, 0, 5, 10]).reshape(-1,1)\n",
    "# data2_gen, targets2 = dw_gen.triplet_dw_gen(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Visualize the triplet data\n",
    "# data2_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Generate data for Quadruplet\n",
    "# data3 = data[data['ptl_idx']==3]\n",
    "# data3_gen, targets3 = dw_gen.quad_dw_gen(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# width_list = np.concatenate([np.linspace(10,3,45), np.linspace(3,10,45)])\n",
    "# plt.plot(data3['dt2'], data3['dw_mean'],'o', label='Raw data')\n",
    "# plt.plot(data3_gen['dt2'], targets3,'o', label='KNN')\n",
    "# targets3_sm = np.concatenate([dw_gen.smooth(targets3[:45],width_list = width_list), dw_gen.smooth(targets3[45:],width_list = width_list)])\n",
    "# plt.plot(data3_gen['dt2'],targets3_sm,'o', label='Smoothness filter')\n",
    "# plt.xlabel('dt(ms)')\n",
    "# plt.ylabel('$\\Delta w$')\n",
    "# plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Sample randomly the smoothed Quadruplet data\n",
    "# samp_len = len(targets3_sm)\n",
    "# np.random.seed(1)\n",
    "# test_idx_quad = np.unique(np.random.randint(low=0, high=90, size=9))\n",
    "# train_vali_idx = np.setdiff1d(np.linspace(0,89,90), test_idx_quad).astype(int)\n",
    "# np.random.seed(10)\n",
    "# vali_idx_idx = np.random.randint(low=0, high=80, size=18)\n",
    "# vali_idx_quad = np.unique(train_vali_idx[vali_idx_idx])\n",
    "# train_idx_quad = np.setdiff1d(train_vali_idx, vali_idx_quad).astype(int)\n",
    "# plt.plot(data3_gen.loc[train_idx_quad]['dt2'],targets3_sm[train_idx_quad],'o', label='train_data')\n",
    "# plt.plot(data3_gen.loc[vali_idx_quad]['dt2'],targets3_sm[vali_idx_quad],'o', label='vali_data')\n",
    "# plt.plot(data3_gen.loc[test_idx_quad]['dt2'],targets3_sm[test_idx_quad],'o', label='test_data')\n",
    "# plt.legend()\n",
    "# print(len(set(train_idx_quad)), len(set(vali_idx_quad)), len(set(test_idx_quad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Visualize kernel\n",
    "from modelval.kernel import KernelGen\n",
    "ker_test = KernelGen()\n",
    "\n",
    "para = trip_para.loc[('Hippo_AlltoAll', 'Full'), :]\n",
    "a = para[:4].values\n",
    "tau = para[4:].values\n",
    "reso_set = 2\n",
    "tau_pre_post = tau[0]/reso_set  # ms\n",
    "tau_post_pre = tau[2]/reso_set # ms\n",
    "\n",
    "ker_test = KernelGen(len_kernel=101)\n",
    "ker_test.trip_model_ker(para, data_name='Hippocampus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_train = pd.concat([data1_gen.loc[train_idx_stdp],data2_gen, data3_gen.loc[train_idx_quad]])\n",
    "targets_gen_train = np.concatenate([targets1_sm[train_idx_stdp], targets2, targets3_sm[train_idx_quad]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_vali = pd.concat([data1_gen.loc[vali_idx_stdp], data2_gen, data3_gen.loc[vali_idx_quad]])\n",
    "targets_gen_vali = np.concatenate([targets1_sm[vali_idx_stdp], targets2, targets3_sm[vali_idx_quad]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_test = pd.concat([data1_gen.loc[test_idx_stdp], data2_gen, data3_gen.loc[test_idx_quad]])\n",
    "targets_gen_test = np.concatenate([targets1_sm[test_idx_stdp], targets2, targets3_sm[test_idx_quad]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# len_stdp = len(vali_idx_stdp)*\n",
    "# len_triplet = len(vali_idx_stdp)*20+len(data2_gen[data2_gen['ptl_idx']==2])*40\n",
    "# len_trip2 = len(vali_idx_stdp)*20+len(data2_gen[data2_gen['ptl_idx']==2])*40+len(data2_gen[data2_gen['ptl_idx']==4])*40\n",
    "# len_quad = len(targets_gen_vali) - len_trip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_gen_train['ptl_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for STDP\n",
    "ptl_list = [1,2,4,3]\n",
    "spk_len = int(data3_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 1\n",
    "aug_times = [20,40,40,20]\n",
    "spk_pairs_train, targets_train = arb_w_gen(df=data_gen_train, ptl_list=ptl_list, targets=targets_gen_train, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for Quadruplet\n",
    "ptl_list = [1,2,4,3]\n",
    "spk_len = int(data3_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 1\n",
    "aug_times = [20,40,40,20]\n",
    "spk_pairs_vali, targets_vali = arb_w_gen(df=data_gen_vali, ptl_list=ptl_list, targets=targets_gen_vali, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spk_pairs_train.shape, spk_pairs_vali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the network\n",
    "ground_truth_init = 0\n",
    "reg_scale=(1, 1)\n",
    "init_seed=(4,5,6,7)\n",
    "toy_data_net = network.TripNet(kernel=ker_test, ground_truth_init=ground_truth_init, init_seed=init_seed, reg_scale=reg_scale, n_input=spk_pairs_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "save_dir= '/src/Plasticity_Ker/model/Trip_ptl1_4_real_aug'\n",
    "# optimizer_op = tf.train.GradientDescentOptimizer\n",
    "toy_net_trainer = trainer.Trainer(toy_data_net.loss, toy_data_net.loss, input_name=toy_data_net.inputs, optimizer_op=optimizer_op, target_name=toy_data_net.target, save_dir=save_dir, optimizer_config={'learning_rate': toy_data_net.lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = dataset.Dataset(spk_pairs_train, targets_train)\n",
    "vali_data = dataset.Dataset(spk_pairs_vali, targets_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "w_post_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "plt.plot(w_pre,  label='ker_pre_trained')\n",
    "plt.plot(w_post,  label='ker_post_trained')\n",
    "plt.plot(w_post_post,  label='ker_post_trained')\n",
    "plt.legend()\n",
    "print(fc_w, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Learn the kernel from random initialization\n",
    "learning_rate = 0.001\n",
    "iterations = 5\n",
    "min_error = -1\n",
    "for i in range(iterations):\n",
    "    toy_net_trainer.train(train_data, vali_data, batch_size=128, min_error=min_error, feed_dict={toy_data_net.lr: learning_rate})\n",
    "    learning_rate = learning_rate/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "toy_net_trainer.restore_best()\n",
    "w_pre = toy_net_trainer.evaluate(ops=toy_data_net.kernel_pre)\n",
    "w_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post)\n",
    "w_post_post = toy_net_trainer.evaluate(ops=toy_data_net.kernel_post_post)\n",
    "fc_w = toy_net_trainer.evaluate(ops=toy_data_net.fc_w)\n",
    "bias = toy_net_trainer.evaluate(ops=toy_data_net.bias)\n",
    "plt.plot(-1*w_pre, label='ker_pre_trained')\n",
    "plt.plot(-1*w_post, label='ker_post_trained')\n",
    "plt.plot(-1*w_post_post, label='ker_post_post_trained')\n",
    "plt.legend()\n",
    "print([fc_w, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Test effect of smoothed kernel\n",
    "# w_pre_sm = w_pre\n",
    "# w_post_sm = w_post \n",
    "# w_post_post_sm = w_post_post\n",
    "# w_pre_sm[:50] = dw_gen.smooth(w_pre[:50], width=2)\n",
    "# w_post_sm[:48] = dw_gen.smooth(w_post[:48], width=2)\n",
    "# w_post_post_sm[:49] = dw_gen.smooth(w_post_post[:49], width=3)\n",
    "\n",
    "# plt.plot(-1 * w_pre_sm)\n",
    "# plt.plot(-1 * w_post_sm)\n",
    "# plt.plot(-1*w_post_post_sm)\n",
    "\n",
    "# ker_test.kernel_pre = w_pre_sm\n",
    "# ker_test.kernel_post = w_post_sm\n",
    "# ker_test.kernel_post_post= w_post_post_sm\n",
    "# ker_test.kernel_scale = fc_w\n",
    "# ker_test.bias = bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compare the target and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ker_test.kernel_pre = w_pre\n",
    "ker_test.kernel_post = w_post\n",
    "# ker_test.kernel_post_post= w_post_post\n",
    "ker_test.kernel_scale = fc_w\n",
    "ker_test.bias = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate training predictions\n",
    "spk_len = int(data1_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_train, spk_len=spk_len, kernel=ker_test, net_type='triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_len = [len(train_idx_stdp), len(data2_gen[data2_gen['ptl_idx']==2]), len(data2_gen[data2_gen['ptl_idx']==4]), len(train_idx_quad)]\n",
    "ptl_whole_len, targets_ptl, predictions_ptl = dw_gen.target_pred_gen(targets_train, predictions, ptl_len, [20, 40, 40, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_name = ['stdp', 'triplet', 'trip2', 'quadruplet']\n",
    "x_fit = np.linspace(np.min(targets_vali)-1, np.max(targets_vali)+1, 100)\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(np.linspace(-30,70,100),np.linspace(-30,70,100), 'k--' )\n",
    "    # R2, corr, y_fit = perform_eval.R2_corr(predictions_ptl[i],targets_ptl[i], x_fit)\n",
    "    plt.plot(targets_ptl[i], predictions_ptl[i], 'o', label=ptl_name[i]+'(n={a})'.format(a=targets_ptl[i].shape[0]))\n",
    "\n",
    "R2, corr, y_fit = perform_eval.R2_corr(predictions,targets_train, x_fit)    \n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.title(''.join(('R2=%.4f'%(R2), ', Corr=%.4f'%(corr))))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for Quadruplet\n",
    "ptl_list = [1,2,4,3]\n",
    "spk_len = int(data3_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 1\n",
    "aug_times = [1,1,1,1]\n",
    "spk_pairs_vali, targets_vali = arb_w_gen(df=data_gen_vali, ptl_list=ptl_list, targets=targets_gen_vali, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate validation predictions\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_vali, spk_len=spk_len, kernel=ker_test, net_type='triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_len = [len(vali_idx_stdp), len(data2_gen[data2_gen['ptl_idx']==2]), len(data2_gen[data2_gen['ptl_idx']==4]), len(vali_idx_quad)]\n",
    "ptl_whole_len, targets_ptl, predictions_ptl = dw_gen.target_pred_gen(targets_vali, predictions, ptl_len, aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_name = ['stdp', 'triplet', 'trip2', 'quadruplet']\n",
    "x_fit = np.linspace(np.min(targets_vali)-1, np.max(targets_vali)+1, 100)\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(np.linspace(-30,70,100),np.linspace(-30,70,100), 'k--' )\n",
    "    # R2, corr, y_fit = perform_eval.R2_corr(predictions_ptl[i],targets_ptl[i], x_fit)\n",
    "    plt.plot(targets_ptl[i], predictions_ptl[i], 'o', label=ptl_name[i]+'(n={a})'.format(a=targets_ptl[i].shape[0]))\n",
    "\n",
    "R2, corr, y_fit = perform_eval.R2_corr(predictions,targets_vali, x_fit)    \n",
    "\n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.title(''.join(('R2=%.4f'%(R2), ', Corr=%.4f'%(corr))))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generat the spike trains and targets for Quadruplet\n",
    "ptl_list = [1,2,4,3]\n",
    "spk_len = int(data3_gen['train_len'].max() * 1000 / ker_test.reso_kernel)\n",
    "if_noise = 0\n",
    "aug_times = [1,1,1,1]\n",
    "spk_pairs_test, targets_test = arb_w_gen(df=data_gen_test, targets=targets_gen_test, ptl_list=ptl_list, if_noise=if_noise, spk_len=spk_len, kernel=ker_test, net_type='triplet', aug_times=aug_times)\n",
    "test= spk_pairs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate test restuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate validation predictions\n",
    "spk_pairs, predictions = arb_w_gen(spk_pairs=spk_pairs_test, spk_len=spk_len, kernel=ker_test, net_type='triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_len = [len(test_idx_stdp), len(data2_gen[data2_gen['ptl_idx']==2]), len(data2_gen[data2_gen['ptl_idx']==4]), len(test_idx_quad)]\n",
    "ptl_whole_len, targets_ptl, predictions_ptl = dw_gen.target_pred_gen(targets_test, predictions, ptl_len, aug_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ptl_name = ['Stdp', 'Triplet', 'Trip2', 'Quadruplet']\n",
    "x_fit = np.linspace(np.min(targets_vali)-1, np.max(targets_vali)+1, 100)\n",
    "for i in range(len(ptl_len)):\n",
    "    plt.plot(np.linspace(-30,70,100),np.linspace(-30,70,100), 'k--' )\n",
    "    # R2, corr, y_fit = perform_eval.R2_corr(predictions_ptl[i],targets_ptl[i], x_fit)\n",
    "    plt.plot(targets_ptl[i], predictions_ptl[i], 'o', label=ptl_name[i]+'(n={a})'.format(a=targets_ptl[i].shape[0]))\n",
    "\n",
    "R2, corr, y_fit = perform_eval.R2_corr(predictions,targets_test, x_fit)    \n",
    "\n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.title(''.join(('R2=%.4f'%(R2), ', Corr=%.4f'%(corr))))\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
